name: Automatización de scraping web

on:
  schedule:
    - cron: '0 0 * * 1'  # Ejecutar todos los lunes
  
  workflow_dispatch:      # Permitir ejecución manual

jobs:

  build:
    runs-on: ubuntu-latest

    env:
      EMAIL_ADDRESS: ${{ secrets.EMAIL_ADDRESS }}
      EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
      DESTINATARIO_EMAIL: ${{ secrets.DESTINATARIO_EMAIL }}
      SCP_USER: ${{ secrets.SCP_USER }}
      SCP_HOST: ${{ secrets.SCP_HOST }}
      SCP_PATH: ${{ secrets.SCP_PATH }}

    steps:

    - name: Checkout code
      run: |
        git clone https://github.com/RickyFer22/Web-Scraping-Automatizado-para-la-Vigilancia-de-Competidores.git .
        git checkout ${{ github.ref }}

    - name: Instalar dependencias
      run: pip install -r requirements.txt

    - name: Instalar Firefox
      run: |
        sudo apt-get update
        sudo apt-get install -y firefox

    - name: Descargar geckodriver
      run: |
        wget https://github.com/mozilla/geckodriver/releases/download/v0.34.0/geckodriver-v0.34.0-linux64.tar.gz
        tar -xvzf geckodriver-v0.34.0-linux64.tar.gz
        chmod +x geckodriver
        sudo mv geckodriver /usr/local/bin/

    - name: Verificar la instalación de geckodriver
      run: geckodriver --version

    - name: Configurar Xvfb
      run: |
        sudo apt-get install -y xvfb
        Xvfb :99 &
        echo "DISPLAY=:99.0" >> $GITHUB_ENV

    - name: Ejecutar scripts de scraping
      run: |
        for file in *.py; do
          python "$file"
        done

    - name: Subir base de datos via scp
      run: scp precios_competencia.db $SCP_USER@$SCP_HOST:$SCP_PATH

    - name: Generar y subir Excel
      run: |
        python 6listas_en_excel.py

    - name: Subir archivo Excel via scp
      run: scp precios_competencia.xlsx $SCP_USER@$SCP_HOST:$SCP_PATH



